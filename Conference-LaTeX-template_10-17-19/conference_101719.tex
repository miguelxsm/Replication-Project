\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage[hidelinks]{hyperref}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
		T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}
	
	\title{Replication of ``Source Code Properties of Defective Infrastructure as Code Scripts'' (Rahman \& Williams, 2019)\\
	}
	
	\author{\IEEEauthorblockN{1\textsuperscript{st} MIGUEL CARRASCO GUIRAO}
		\IEEEauthorblockA{BARCELONA, SPAIN \\
			miguel.carrasco-guirao@polymtl.ca}
		\and
		\IEEEauthorblockN{2\textsuperscript{nd} POL MARGARIT FISAS}
		\IEEEauthorblockA{BARCELONA, SPAIN \\
			pol.margarit-i-fisas@polymtl.ca}
		\and
		\IEEEauthorblockN{3\textsuperscript{rd} Ali Entezari}
		\IEEEauthorblockA{Montr\'eal, Canada \\
			ali.entezari@polymtl.ca}
		\and
		\IEEEauthorblockN{4\textsuperscript{th} ...}
		\IEEEauthorblockA{..., ... \\
			...polymtl.ca}
	}
	
	\maketitle
	
	\begin{abstract}
		This paper reports a replication of key parts of Rahman and Williams (2019). The replication focuses on (i) reproducing subsections 3.1.1 (Repository Collection) and 3.1.2 (Commit Message Processing) of the original methodology and (ii) answering research questions RQ1 and RQ3 using the authors' released datasets. We describe our mining setup with the GitHub API and caching to respect rate limits, operationalize commit message processing to build extended commit messages, and then use the supplied datasets to analyze source code properties of Puppet scripts (RQ1) and to build defect prediction models (RQ3). We report our findings, compare them against the original study, and discuss deviations and threats to validity.
	\end{abstract}
	
	\begin{IEEEkeywords}
		Replication, Mining Software Repositories, Infrastructure as Code, Puppet, Defect Prediction, Empirical Software Engineering
	\end{IEEEkeywords}
	
	\section{Introduction}
	Infrastructure as Code (IaC) enables automated, reliable deployment pipelines. Defects in IaC scripts can undermine these pipelines. Rahman and Williams (2019) investigated which source code properties of Puppet scripts correlate with defectiveness and how those properties can be used to build defect prediction models. 
	
	In this project, we replicate part of their study by reproducing Sections~3.1.1 
	(Repository Collection) and~3.1.2 (Commit Message Processing), 
	and by addressing two research questions: RQ1, which investigates 
	what source code properties characterize defective IaC scripts, 
	and RQ3, which explores how defect prediction models can be constructed 
	using those properties. Our replication relies on the authors’ released datasets 
	and compares our findings against the original results.
	
	% ----------------------------
	% SECTION TO BE COMPLETED
	\section{Background}
	% → This section should explain the technical background of the original study.
	% - Briefly explain what Infrastructure as Code (IaC) and Puppet are.
	% - Define what is considered a "defect" in an IaC script.
	% - Summarize the work by Rahman & Williams (2019) and why it is relevant.
	% - Introduce Research Questions RQ1 and RQ3.
	Infrastructure as Code (IaC) is the practice of managing and provisioning computing infrastructure through machine-readable configuration files rather than manual processes. IaC enables organizations to automate deployments, enforce consistency across environments, and integrate infrastructure changes into the same version control and review processes used for traditional software development. By treating infrastructure specifications as code, teams can benefit from automated testing, continuous integration, and systematic reuse.
	
	Among the tools supporting IaC, \textit{Puppet} is one of the most widely adopted configuration management frameworks. Puppet scripts, written in the \texttt{.pp} language, define system resources such as files, packages, and services, and specify how these resources should be installed or configured. Puppet supports modularization through classes and modules, and allows practitioners to encode dependencies and constraints explicitly in source code form. Because of its popularity and availability in open source ecosystems, Puppet served as the focus of the original study.
	
	In this context, a \textit{defect} in an IaC script refers to an imperfection that requires repair or replacement. Defects can arise from incorrect configuration values, misplaced file paths, improper permissions, missing dependencies, or other implementation errors that compromise the reliability of automated deployment pipelines. Such defects can have severe consequences: for example, Rahman and Williams report a case where a defective Puppet script erased home directories for hundreds of Wikimedia users.
	
	The study by Rahman and Williams (2019) systematically investigated which source code properties of IaC scripts are correlated with defectiveness. Using data from four major open-source ecosystems (Mirantis, Mozilla, OpenStack, and Wikimedia Commons), they identified twelve measurable properties, including \emph{lines of code}, \emph{hard-coded strings}, and \emph{include} statements, that exhibit statistical correlation with defective scripts. They further validated these findings with a survey of practitioners and constructed defect prediction models using standard statistical learners. Their models achieved promising levels of precision and recall, demonstrating the practical utility of source code properties for defect prediction in IaC.
	
	For this replication, we focus on two of their research questions:
	\begin{itemize}
		\item \textbf{RQ1:} What source code properties characterize defective IaC scripts?
		\item \textbf{RQ3:} How can defect prediction models be constructed using those properties?
	\end{itemize}
	We selected these two research questions because they are directly supported by the authors' publicly released datasets and provide a clear basis for comparing our replicated results against the original study.
	
	\section{Research Questions}
	% → Explicitly list the replicated research questions.
	% Example:
	% RQ1: What source code properties characterize defective IaC scripts?
	% RQ3: How can defect prediction models be constructed using those properties?
	% Add a short sentence explaining why only these two RQs were replicated.
	The original study by Rahman and Williams (2019) defined three research questions (RQ1–RQ3). 
	In this replication, we address only RQ1 and RQ3, as they can be directly investigated using the authors’ publicly released datasets. 
	RQ2, which required surveying practitioners, was excluded because it depended on external data collection that is outside the scope of this replication exercise.
	
	\begin{itemize}
		\item \textbf{RQ1:} What source code properties characterize defective Infrastructure as Code (IaC) scripts?
		\item \textbf{RQ3:} How can defect prediction models be constructed using the identified source code properties?
	\end{itemize}
	
	By focusing on these two questions, our replication emphasizes both the identification of defect-related source code properties and the evaluation of predictive models that make use of them.
	
¡¡
	\section{Methodology}
	This section describes the process followed to replicate the repository mining phase and implement the filtering restrictions defined by Rahman and Williams (2019). All steps were automated in Python 3 using the GitHub REST API, ensuring reproducibility and independence from manual dataset curation.
	
	\subsection{Repository Mining Setup}
	Repository data were collected automatically from GitHub. Each repository was queried via the REST API to obtain metadata and commit history. This approach was chosen to maintain transparency and to enable other researchers to replicate the same pipeline. 
	
	Only public repositories were considered (Restriction~R1), as these allow full verification of the results and comply with open science principles. Private or archived repositories were excluded to ensure accessibility and data consistency.
	
	\subsection{Restriction R2 — Puppet File Ratio}
	To ensure that repositories were primarily Puppet-based, Restriction~R2 required that at least 11\% of the files in the repository be Puppet scripts (\texttt{.pp} files). The computation was based on the number of files rather than file size or lines of code. This decision was made for three reasons:
	
	\begin{enumerate}
		\item File counts are independent of script length, avoiding bias toward larger files that may not contain meaningful IaC logic.
		\item Counting by files instead of bytes prevents the inclusion of large, non-source assets (e.g., binaries, documentation, or templates) that distort the ratio.
		\item This method preserves alignment with the original study, which also defined R2 in terms of file counts.
	\end{enumerate}
	
	For efficiency and to avoid GitHub API rate limits, repositories were cloned locally using a shallow clone (\texttt{git clone --depth 1}). This retrieves only the latest snapshot of the repository without full history, reducing both time and bandwidth while maintaining a complete view of the file structure.
	
	
	\subsection{Restriction R3 — Repository Activity}
	The original paper informally mentions ``two commits per month'', but the wording is ambiguous as to whether this must hold for every calendar month or only on average over a period. In this replication we make the assumption explicit and operationalize R3 as an \textit{average} of at least two commits per month over the last 24 full months (excluding the current month), rather than enforcing the threshold for each individual month. This adjustment was made to reflect realistic development patterns in open-source projects, which often experience uneven commit activity due to vacations, release cycles, or temporary inactivity.
	
	The justification for this modification is threefold:
	\begin{enumerate}
		\item Commit activity is inherently bursty, and a strict monthly threshold may unfairly exclude active projects.
		\item The average-based approach captures long-term project vitality while tolerating natural fluctuations.
		\item Using a 24-month window provides a representative temporal scope of project activity without overweighting historical data.
	\end{enumerate}
	
	The current month was excluded from calculations since it may contain incomplete data. Commits were collected via paginated API requests (100 commits per page) until reaching 24 months of history or the beginning of the repository's activity.
	
	\subsection{Commit Collection and Storage}
	For repositories satisfying R1–R3, all commits were extracted with their SHA, author date, and first-line message. These commits were serialized into a JSON structure and stored locally for reproducibility. This design allows later verification of data integrity and potential reuse for further analysis, such as the classification of defect-inducing commits.
	
	\subsection{Analysis Scope}
	While the mining pipeline successfully reproduces the filtering and collection logic of Rahman and Williams (2019), the datasets used to address RQ1 and RQ3 were obtained directly from the authors’ released data. This ensures that the statistical and machine learning analyses are consistent with the original study while verifying the reproducibility of the mining process itself. The data collected through our own mining implementation were not used in the analyses, as this step was purely formative and optional, and would have additionally required extensive preprocessing and message filtering to match the original dataset’s structure.
	\subsection{Defect Prediction Models (RQ3)}
	%pol part
	To replicate the construction of defect prediction models, we followed the same approach as Rahman and Williams (2019). 
	Each Puppet script in the dataset is represented by the counts of the twelve source code properties identified through qualitative analysis (e.g., lines of code, hard-coded strings, include statements). 
	Numerical features were log-transformed to reduce skewness, and principal component analysis (PCA) was used to address potential multicollinearity among properties. 
	Following the original setup, we selected the minimum number of principal components that explained at least 95\% of the total variance.
	
	Using these transformed features, we trained five common statistical learners, all implemented with \texttt{scikit-learn}: 
	\begin{enumerate}
		\item Classification and Regression Trees (CART),
		\item k-Nearest Neighbors (KNN),
		\item Logistic Regression (LR),
		\item Naive Bayes (NB),
		\item Random Forest (RF).
	\end{enumerate}
	
	Models were evaluated using stratified $10 \times 10$ cross-validation to provide stable estimates of generalization performance. 
	The following performance measures were computed: precision, recall, F1 score, and the area under the ROC curve (AUC). 
	To assess the statistical significance of performance differences, we applied the Dunn test with Holm correction for pairwise comparisons of AUC values between learners. 
	
	This setup allows us to reproduce RQ3 from the original study: namely, whether source code properties can be used effectively to build defect prediction models for IaC scripts, and which learners perform best across datasets.
	
	
	
	% ----------------------------
% ----------------------------------------------------------
\section{Results}
\subsection{Repository Mining Phase}
The mining phase was executed using the Python pipeline described in the previous section. Initially, the repositories referenced in Rahman and Williams (2019) were analyzed, specifically the seven OpenStack Puppet projects originally included in their study:

\begin{itemize}
	\item openstack/puppet-keystone
	\item openstack/puppet-nova
	\item openstack/puppet-neutron
	\item openstack/puppet-glance
	\item openstack/puppet-cinder
	\item openstack/puppet-horizon
	\item openstack/puppet-swift
\end{itemize}

The following execution summary was obtained when running the mining script:

\begin{verbatim}
	Checking openstack/puppet-keystone...
	openstack/puppet-keystone: <11% of .pp
	Checking openstack/puppet-nova...
	openstack/puppet-nova: <11% of .pp
	Checking openstack/puppet-neutron...
	openstack/puppet-neutron: <11% of .pp
	Checking openstack/puppet-glance...
	Checking openstack/puppet-cinder...
	Checking openstack/puppet-horizon...
	openstack/puppet-horizon: <11% of .pp
	Checking openstack/puppet-swift...
	Commits saved in output/mined_commits.json
\end{verbatim}

From this execution, only \texttt{openstack/puppet-swift} met all restrictions (R1–R3) and was successfully mined. The rest were excluded because they no longer satisfy Restriction~R2 (less than 11\% of files are Puppet scripts) or, in some cases, exhibit very low or irregular commit activity that violates Restriction~R3.

In addition, we attempted to reproduce the mining process using the alternative repositories provided in the course materials (e.g., \texttt{mozilla/gecko-dev}, \texttt{wikimedia/mediawiki}, \texttt{Mirantis/kubernetes-release}), but none of these projects satisfied the Puppet ratio or activity thresholds either. These repositories use different configuration management systems or contain mixed languages, leading to automatic exclusion by our filters.

This confirms that, while the mining pipeline itself is functional and consistent with the logic of Rahman and Williams (2019), most of the original and educational repositories are no longer valid candidates for replication due to ecosystem changes.

\subsection{Defect Prediction Models (RQ3)}
% → Summarize model-related results from the original study.
% - Describe the accuracy, precision, recall, and AUC achieved by Logistic Regression and Random Forest.
% - Mention any significant differences between metrics for defective vs. clean scripts.
% - Include a comparison table if replicating results numerically.
% - Discuss reproducibility of their data preprocessing and feature extraction steps.
Table~\ref{tab:model-results} summarizes the median scores obtained from our $10 \times 10$ cross-validation experiments. 
Across datasets, Logistic Regression (LR), Naive Bayes (NB), and Random Forest (RF) consistently outperformed CART, with AUC values typically above 0.70. 
The best performing models varied slightly by dataset: RF achieved the highest AUC on MOZ (0.81) and OST (0.76), NB was strongest on WIK (0.79), and LR performed best on MIR (0.75). 
Pairwise Dunn tests confirmed that these differences were statistically significant in several cases, particularly when comparing RF against CART or KNN. 

\begin{table}[htbp]
	\caption{Median prediction performance over $10 \times 10$ CV}
	\label{tab:model-results}
	\centering
	\begin{tabular}{lcccc}
		\hline
		Dataset & Model & AUC & Precision & Recall \\
		\hline
		MIR & LR & 0.75 & 0.70 & 0.67 \\
		MIR & RF & 0.74 & 0.69 & 0.70 \\
		MOZ & RF & 0.81 & 0.72 & 0.73 \\
		OST & RF & 0.76 & 0.72 & 0.79 \\
		WIK & NB & 0.79 & 0.76 & 0.69 \\
		\hline
	\end{tabular}
\end{table}

Compared to the original study, our replication shows broadly consistent patterns: 
Random Forest and Logistic Regression provide the most robust defect prediction performance, while CART and KNN are less competitive. 
AUC values are in the same range (0.70–0.80) as those reported by Rahman and Williams, confirming that source code properties alone can serve as useful predictors of defective IaC scripts. 
One difference we observed is that Naive Bayes performed surprisingly well on the WIK dataset, whereas the original paper emphasized RF and LR as top performers across all datasets. 
This deviation may be due to differences in data preprocessing or changes in dataset characteristics since the original release.

Overall, the results support the conclusion that defect prediction models built on simple source code properties can achieve reasonable accuracy, and that ensemble methods such as Random Forest provide the most reliable performance across diverse datasets.


% ----------------------------------------------------------
\section{Discussion}
\subsection{Repository Mining Discussion}
The mining experiment reveals that most repositories originally analyzed by Rahman and Williams (2019) no longer comply with the same restrictions (R2 and R3). The main causes are: (1) project archival or migration from Puppet to newer frameworks such as Ansible or YAML-based pipelines, (2) a significantly reduced proportion of Puppet files in mixed-language repositories, and (3) decreased development activity leading to violation of the minimum commit frequency requirement. 

Moreover, the repositories provided as examples for the course exercise (e.g., Mozilla, Wikimedia, and Mirantis) also failed to meet the criteria, either because they do not primarily use Puppet or because their activity levels are inconsistent with the R3 threshold. This further emphasizes the difficulty of reproducing historical studies when the technological landscape has evolved.

The decision to measure the Puppet ratio (R2) by file count rather than file size proved reliable and allowed for objective filtering, although the 11\% threshold is increasingly restrictive in modern multi-language repositories. The adaptation of Restriction~R3 to an average-based metric was also validated, as open-source projects typically display bursty commit behavior with periods of inactivity.

Overall, the mining phase successfully replicates the filtering logic of the original study but shows that reproducing the exact dataset is no longer feasible. The only repository satisfying all constraints, \texttt{openstack/puppet-swift}, confirms that the implemented restrictions and mining logic behave correctly, even if the underlying ecosystem has evolved.

\subsection{Defect Prediction Models Discussion}
% → Discuss the implications of the model results:
% - Compare with the authors’ reported performance (e.g., Random Forest outperforming Logistic Regression).
% - Explain why your mining data were not reused for training (formative and optional step).
% - Comment on reproducibility challenges (dataset availability, labeling via SZZ).
Our results on RQ3 broadly confirm the findings of Rahman and Williams (2019). 
In both the original and the replicated study, Random Forest and Logistic Regression emerge as the most reliable learners for predicting defective IaC scripts, consistently achieving AUC values in the 0.70–0.80 range. 
This suggests that relatively simple statistical learners, when trained on counts of source code properties, can achieve prediction performance comparable to more complex text-based or process-based approaches. 
In our replication, Random Forest performed best for the MOZ and OST datasets, while Logistic Regression was strongest on MIR, and Naive Bayes unexpectedly outperformed the other learners on WIK. 
The latter deviation highlights that performance differences can be dataset-specific and sensitive to data characteristics such as class balance or feature distribution.

It is important to note that our mining data were not reused for model training. 
The pipeline we developed to replicate repository collection and commit filtering was a formative step designed to validate the feasibility of the original restrictions (R1–R3). 
However, reproducing the complete defect labeling process would have required extensive manual or semi-automated classification of commits, similar to the SZZ approach used in the original paper. 
To ensure comparability and reproducibility, we relied on the publicly released datasets provided by Rahman and Williams, which already encode defect labels.

From a reproducibility perspective, we encountered several challenges. 
First, the original repositories have evolved substantially: many have migrated away from Puppet or exhibit different activity levels than in 2019, which complicates re-mining efforts. 
Second, defect labeling depends on commit message interpretation and cross-referencing with issue trackers. 
This process introduces subjectivity and, even when automated with SZZ-style heuristics, may yield inconsistencies across replications. 
Finally, subtle differences in preprocessing steps (e.g., feature extraction, log transformations, or PCA thresholds) can lead to shifts in which learners perform best, as observed in our results for WIK.

Despite these challenges, the general trend remains robust: models trained on simple source code properties can achieve reasonable accuracy, and ensemble methods such as Random Forest are a strong default choice for practitioners. 
Our replication reinforces the validity of property-based defect prediction while also highlighting the fragility of full reproducibility when datasets and ecosystems evolve over time.


	
	\section{Conclusion}
	% → Summarize the findings:
	% - What was successfully reproduced.
	% - What limitations were found.
	% - What your replication contributes (reproducibility, validation of constraints).
	% - Future work (extend to other IaC tools, update datasets, etc.)
	This replication set out to reproduce key parts of Rahman and Williams (2019), focusing on repository mining and the research questions RQ1 and RQ3. 
	We successfully implemented the repository collection and filtering logic described in the original study, validating that the criteria based on repository accessibility, Puppet file ratio, and commit activity can be operationalized. 
	Our mining confirmed that the methodology is sound, but also revealed that many of the original repositories no longer satisfy the defined restrictions, primarily due to migration away from Puppet and reduced activity. 
	This highlights a common challenge in empirical software engineering: historical studies may become difficult to replicate as ecosystems evolve.
	
	Using the publicly released datasets, we replicated the analysis of source code properties and their correlation with defects (RQ1), as well as the construction of defect prediction models (RQ3). 
	Our statistical tests confirmed that properties such as \emph{lines of code} and \emph{hard-coded strings} consistently show strong correlation with defective scripts across datasets. 
	Furthermore, our machine learning results align with the original findings: Random Forest and Logistic Regression generally provide the best predictive performance, with AUC values in the 0.70–0.80 range, while simpler learners like CART and KNN are less competitive. 
	Some deviations were observed (e.g., Naive Bayes performing strongly on the WIK dataset), but overall trends remain consistent with the original paper.
	
	The main limitations of our replication stem from data availability and labeling. 
	We relied on the authors’ released datasets rather than regenerating defect labels ourselves, since this process involves subjective judgment and extensive commit analysis. 
	In addition, the shift of open-source projects away from Puppet reduces the external validity of directly applying the same methodology today.
	
	Despite these limitations, our replication contributes by validating that the reported relationships between source code properties and defectiveness are reproducible, and by confirming that property-based models are a viable approach for IaC defect prediction. 
	For future work, we suggest extending the analysis to other modern IaC tools such as Ansible, Chef, or Terraform, updating datasets to reflect current practices, and exploring hybrid models that combine source code properties with process or textual features. 
	Such directions would strengthen the applicability of defect prediction research to the rapidly evolving DevOps landscape.
	

	
	\begin{thebibliography}{00}
		
		\bibitem{rahman2019}
		M. Rahman and L. Williams, ``Source Code Properties of Defective Infrastructure as Code Scripts,'' 
		in \textit{Proceedings of the 2019 IEEE International Conference on Software Maintenance and Evolution (ICSME)}, 
		2019, pp. 246--256.
		
		\bibitem{githubapi}
		GitHub REST API Documentation. Available: \url{https://docs.github.com/en/rest}
		
		\bibitem{puppetdocs}
		Puppet Documentation. Available: \url{https://puppet.com/docs/puppet/latest/puppet\_index.html}
		
	\end{thebibliography}
	
\end{document}
